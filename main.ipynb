{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902cc1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9778281bf242d49317d53ea11c58e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed683f0104dc42888b64456a5e84bc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0729fa539419fa1dde23de3236744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de05bdcc47e4499a82b29c3298689434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99a01cd62bd74266beafa6ce13fbe340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from pipeline_stable_diffusion_3 import StableDiffusion3Pipeline\n",
    "from transformer_sd3 import SD3Transformer2DModel\n",
    "from module import Module\n",
    "transformer = SD3Transformer2DModel.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16, subfolder=\"transformer\")\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3.5-large\", torch_dtype=torch.bfloat16)\n",
    "pipe.transformer = transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b33d10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5b7acd02c8402dab906ee1a13cd8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipe.to(\"cuda\")\n",
    "prompt = \"A cup with a dog photo on it\"\n",
    "negative_prompt=\"sky\"\n",
    "module = Module().cuda()\n",
    "image1 = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=4,\n",
    "    module=module\n",
    ").images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b422fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "intermediate_latents1 = copy.deepcopy(pipe.intermediate_latents)\n",
    "intermediate_prompt_embeds1 = copy.deepcopy(pipe.intermediate_prompt_embeds)\n",
    "pred1 = copy.deepcopy(pipe.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1383796f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19e331c8ea648c289fd84fa9c0d3a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipe.to(\"cuda\")\n",
    "prompt = \"A cup with a dog photo on it\"\n",
    "negative_prompt=\"sky\"\n",
    "image2 = pipe(\n",
    "    prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=4,\n",
    "    module=module\n",
    ").images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965e7b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20.0, 13.0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from judge import ask_gpt \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "ask_gpt(image1, image2, prompt, negative_prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2f25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_latents2 = copy.deepcopy(pipe.intermediate_latents)\n",
    "intermediate_prompt_embeds2 = copy.deepcopy(pipe.intermediate_prompt_embeds)\n",
    "pred2 = copy.deepcopy(pipe.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f287df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.pred[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10d4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "torch.Size([20, 1024]) torch.Size([1024])\n",
      "Term: -0.07855579303577542\n",
      "0.9244504805939462\n"
     ]
    }
   ],
   "source": [
    "def compute_term(module, intermediate_latents, intermediate_prompt_embeds, pred):\n",
    "    term = 0\n",
    "    \n",
    "    for i in range(len(intermediate_latents)):\n",
    "        latents = intermediate_latents[i].cuda().float().unsqueeze(0)\n",
    "        prompt_embeds = intermediate_prompt_embeds[i].cuda().float().unsqueeze(0)\n",
    "        logistic = module(latents, prompt_embeds)\n",
    "        prob = torch.softmax(logistic, dim=1) \n",
    "        # print(prob[0,:,10,10].argmax().item(), pred[i][:,10,10].item(), prob[0,:,10,10].max().item())\n",
    "        prob = prob.reshape(prob.shape[1], -1) \n",
    "        pred_ = pred[i].reshape(-1)\n",
    "        # print(prob.shape, pred_.shape)\n",
    "        # selected_prob = prob[pred_] \n",
    "        selected_prob = torch.gather(prob, 0, pred_.unsqueeze(0).long()).squeeze(0)\n",
    "        selected_log_prob = torch.log(selected_prob + 1e-10)\n",
    "        term += torch.mean(selected_log_prob).item()\n",
    "        # print(torch.mean(selected_log_prob).item())\n",
    "    print(f\"Term: {term/len(intermediate_prompt_embeds)}\")\n",
    "    print(f\"{np.exp(term/len(intermediate_prompt_embeds))}\")\n",
    "compute_term(module, intermediate_latents1, intermediate_prompt_embeds1, pred1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
